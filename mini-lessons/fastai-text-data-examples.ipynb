{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/MLT-x-fastai/blob/master/mini-lessons/fastai_text_data_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8F6rk1YwJ9P"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btS8qaALwJ9b"
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZOWlQymwJ9g"
   },
   "source": [
    "Content from this notebook has been inspired from the fast.ai course-v3 Part 1. In this notebook we'll go through some standard NLP tasks using lessons from fast.ai. We will learn about Transfer Learning in NLP, creating `TextDataBunch` objects, training a model for text classification task and a Sequence2Sequence task We will see how the `fastai` library uses a style called the Data Block API to simplify the process of prepping data for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RAAKTmvMwJ9i"
   },
   "outputs": [],
   "source": [
    "#Google Colab notes: imports for uploading files\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQMt1sO6wUHO"
   },
   "outputs": [],
   "source": [
    "#Google Colab notes: make data directory\n",
    "#!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbtcGVLWwJ9p"
   },
   "outputs": [],
   "source": [
    "path = Path('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "de1RHLVWwJ9t"
   },
   "source": [
    "## Language Modeling\n",
    "\n",
    "One of the most common tasks done in NLP is called language modeling. A language model is an NLP model which learns to predict the next word in a sentence. We do this is because we assume that if a language model is quite accurate at guessing the next probable word in a sentnce, it needs a lot of world knowledge and a deep understanding of grammar, semantics, and other elements of natural language.\n",
    "\n",
    "We will show how to train a simple language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "562r2FPmwJ9u",
    "outputId": "147d533d-32cc-4e59-b6e7-3c2949137bb1"
   },
   "outputs": [],
   "source": [
    "#!wget https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
    "#!mv nietzsche.txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "TvqPqmt0wJ91",
    "outputId": "23b97728-6ca4-4e29-b1ba-3b7c813cbcb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PREFACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nSUPPOSING that Truth is a woman--what then? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sils Maria Upper Engadine, JUNE, 1885.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nCHAPTER I. PREJUDICES OF PHILOSOPHERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                            PREFACE\n",
       "1  \\nSUPPOSING that Truth is a woman--what then? ...\n",
       "2             Sils Maria Upper Engadine, JUNE, 1885.\n",
       "3                                                   \n",
       "4            \\nCHAPTER I. PREJUDICES OF PHILOSOPHERS"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(open(path/'nietzsche.txt').read().split('\\n\\n'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0LSpb-SwJ97"
   },
   "outputs": [],
   "source": [
    "bs=64\n",
    "data_lm = (TextList.from_df(df,path,cols=[0])\n",
    "                   .split_by_rand_pct(0.1)\n",
    "                   .label_for_lm()\n",
    "                   .databunch(bs=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "JJf8PW9cwJ-A",
    "outputId": "5ec5e34e-faca-4a35-af42-0b998aec5960"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>unseemly methods for \\n  xxunk a woman ? xxmaj certainly she has never allowed herself to be won ; and \\n  at present every kind of dogma stands with sad and xxunk xxunk -- xxup if , \\n  indeed , it stands at all ! xxmaj for there are xxunk who maintain that it \\n  has fallen , that all dogma lies on the ground --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>an opinion is life - xxunk , life - preserving , \\n  species - preserving , perhaps species - rearing , and we are fundamentally \\n  inclined to maintain that the xxunk opinions ( to which the synthetic \\n  judgments a priori belong ) , are the most indispensable to us , that \\n  without a recognition of logical fictions , without a comparison of \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>, the \\n  will to \" creation of the world , \" the will to the causa xxunk . xxbos 10 . xxmaj the eagerness and subtlety , i should even say craftiness , with \\n  which the problem of \" the real and the apparent world \" is xxunk with at \\n  present throughout xxmaj europe , furnishes food for thought and attention ; and \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>belief ought to be xxunk from science ! xxmaj between \\n  ourselves , it is not at all necessary to get rid of \" the soul \" thereby , \\n  and thus renounce one of the oldest and most xxunk hypotheses -- as \\n  happens frequently to the clumsiness of xxunk , who can hardly \\n  touch on the soul without immediately losing it . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>alone is really known to us , absolutely and \\n  completely known , without deduction or addition . xxmaj but it again and \\n  again seems to me that in this case xxmaj schopenhauer also only did what \\n  philosophers are in the habit of doing -- he seems to have adopted a \\n  xxup popular xxup prejudice and exaggerated it . xxmaj willing seems to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgctHNM_wJ-K"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM,drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "mKZQVe88wJ-S",
    "outputId": "79059f27-394d-4a69-fc8d-c08d3d3f02d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKDFAoiIjJIoSAiIoP+P50J1sOX+8uCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "VH1Rrf2mwJ-a",
    "outputId": "0debccd6-8716-4222-fab2-110bdc704210"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.749605</td>\n",
       "      <td>4.329587</td>\n",
       "      <td>0.213988</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1,1e-2,moms=(0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "lLQSsIHZwJ-h",
    "outputId": "fea85c45-44ac-45d8-b8f5-1c631e0063bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.242284</td>\n",
       "      <td>3.997777</td>\n",
       "      <td>0.256920</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.024162</td>\n",
       "      <td>3.899869</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.843143</td>\n",
       "      <td>3.868506</td>\n",
       "      <td>0.266295</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.600221</td>\n",
       "      <td>3.874033</td>\n",
       "      <td>0.270610</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4,3e-3,moms=(0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "_Djk9URLwJ-m",
    "outputId": "4e0f7e22-ca27-4d01-9ce9-3c60db7c2c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is perhaps just dawning on five or six minds . Yet the most extensive fact of the \\n  circumstances has been been provided after the accident of More \\n  and Bell of Saint fire in whose grace and hatred of \\n  period talk represents their goal : where the populace has grown \\n  and there is well enough with regard to the HIGHER \\n  world of the present school , in whom a number could not , step home , be learnt \\n  to live as maid and lover and of distinction : \\n  hitherto , of course , in speaking hours and habits'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('It is perhaps just dawning on five or six minds',n_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqQ2WuWKwJ-u"
   },
   "source": [
    "## Tokenization and Numericalization\n",
    "The most common preprocessing on NLP tasks in tokenization i.e splitting the sentence into words. This is much easier in space-seperated words like English however, for Japanese we require Morphological Analysis tools to get words from sentences.\n",
    "\n",
    "Numericalizing in the second preprocessing step. Since models can only take numbers as inputs, we make a dictionary mapping unique words to indices and replace the words with the words in the sentence with their corresponding index. Here we limit our dictionary size to 60000 words that appear at least twice in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nppsyhBqxQyT"
   },
   "outputs": [],
   "source": [
    "#Google Colab notes: install dependencies for mecab tokenizer\n",
    "#!sudo apt install swig\n",
    "#!sudo apt install mecab\n",
    "#!sudo apt install libmecab-dev\n",
    "#!sudo apt install mecab-ipadic-utf8\n",
    "#!sudo pip3 install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZvJ9IAkxcGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZKaNpATwJ-u"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "class MeCabTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        self.lang = 'ja'\n",
    "    \n",
    "    def add_special_cases(self, toks:Collection[str]): pass\n",
    "    \n",
    "    def tokenizer(self,raw_sentence):\n",
    "        result = tagger.parse(raw_sentence)\n",
    "        words = result.split()\n",
    "        if len(words) == 0:\n",
    "            return []\n",
    "        if words[-1] == \"\\n\":\n",
    "            words = words[:-1]\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5t-GgbaowJ_K"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(MeCabTokenizer, 'ja')\n",
    "processor = [TokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(max_vocab=60000,min_freq=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7BF02EpwJ-y"
   },
   "source": [
    "## Text Classification\n",
    "One particular area that was challenging until recently with deep learning for NLP, was text classification.\n",
    "\n",
    "Similar to classifying images in text we can also use transfer learning to train accurate classifiers with few training examples. We will leverage weights from a language model trained on a large corpus as our pretrained weights. We will fine-tune the language model to our target dataset, attach a classification layer to our model and train by gradual unfreezing. The text classifying task will be sentiment analysis on Yahoo Movie Reviews. The data can be downloaded from this [repository](https://github.com/dennybritz/sentiment-analysis/tree/master/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Ndi1UyJwJ-0"
   },
   "outputs": [],
   "source": [
    "#Google Colab Notes: download data and upload them into your notebook environment\n",
    "#files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udgse3lozEiv"
   },
   "outputs": [],
   "source": [
    "#Google Colab Notes: uncompress data and move to data directory\n",
    "#!tar xvzf yahoo-movie-reviews.json.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URd13SIGwJ-3"
   },
   "outputs": [],
   "source": [
    "def load_ymr_data(path):\n",
    "    with path.open() as f:\n",
    "        data = pd.read_json(f)\n",
    "        data.movieName = data.movieName.str.strip()\n",
    "        data.text = data.text.str.strip()\n",
    "        data.title = data.title.str.strip()\n",
    "        data = data[data.text.str.len() > 0]\n",
    "        data.url = data.url.str.strip()\n",
    "    return data\n",
    "\n",
    "def make_polar(data, balance=True):\n",
    "    data_polar = data.loc[data.rating != 3].copy()\n",
    "    data_polar.loc[data_polar.rating <= 2, 'rating'] = 0\n",
    "    data_polar.loc[data_polar.rating >= 4, 'rating'] = 1\n",
    "    if balance:\n",
    "        # Subsample - We want the same number of positive and negative examples\n",
    "        grouped_ratings = data_polar.groupby('rating')\n",
    "        K = grouped_ratings.rating.count().min()\n",
    "        indices = itertools.chain(\n",
    "            *[np.random.choice(v, K, replace=False) for k, v in grouped_ratings.groups.items()])\n",
    "        data_polar = data_polar.reindex(indices).copy()\n",
    "    return data_polar\n",
    "\n",
    "\n",
    "mov_df = load_ymr_data(path/'yahoo-movie-reviews.json')\n",
    "mov_df_polar = make_polar(mov_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "8jTE7YgUwJ_A",
    "outputId": "5028f518-38ae-42de-b319-44f57c04a054"
   },
   "outputs": [],
   "source": [
    "#mov_df_polar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zXLt6RmwJ_R"
   },
   "outputs": [],
   "source": [
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjtxfW2RwJ_Y"
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(mov_df_polar,path,cols=['text'],processor=processor)\n",
    "                   .split_by_rand_pct(0.1)\n",
    "                   .label_for_lm()\n",
    "                   .databunch(bs=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "8VFZlZJRwJ_g",
    "outputId": "8403edce-33d6-41b3-be0d-c62912449e7b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>とか 、 好き か 嫌い か という 以前 に 、 映画 の 質 そのもの に 疑問 。 日本映画 史 に 残っ て いく はず の 作品 だけ に … … 、 残念 。 とにもかくにも 、 演出 と 脚本 が ひど すぎる 。 言い出し たら 切り が ない ぐらい だ が 、 まずは 、 なんと いう か 、 「 古い 」 。 （ あるいは 、 いわゆる 「 ベタ 」 。 ）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>感情 移入 する 人 を 選ぶ な～ と 思い ます 。 いい 子 ばかり が 出る 登場人物 は 、 盛り上がり に も かけ 、 当時 の 洗濯機 や 台所 まわり の 様子 、 風景 を 見 て 、 「 懐かしい わ～ 」 や 「 あの 頃 は ～」 という こと しか 残ら ない 。 テレビ の 2時間 の 枠 の アニメ で 充分 です 。 まわり の 子ども さん は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>に は 「 やめ た 方 が いい ！ 」 と 声 を 大 に し て 言い たい ！ ！ 大人 も 子供 も 楽しめ ない 映画 だ と 、 私 は 思い ます ！ 最後 に 余談 です が 、 「 xxunk 」 で この 映画 を 紹介 し た とき に 、 佐 ○ 木 ｱﾅ が 「 私 は これ を 観 て \" xxup et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>の 、 偏見 という の は 多かれ 少なかれ ある と 思い ます 。 自分 達 が 美化 さ れ 過ぎ それ に 、 いくら 不遇 な 過去 へ の 孤独感 、 人 と 上手く コミュニケーション が でき ない もどかし さ が ある と は いえ あんな 形 で 、 聾唖 少女 の 痛み を 演じ させる なんて 。 健常者 より も 、 障害 を 持た れる 方 は 、 ずっと</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>終わり ？ まじ か よ 。 」 「 なに これ ？ こんなに 長い 時間 かけて こんな 終わり ？ 」 って 声 が かなり あり まし た 。 来年 の ４月 に パート ２ が やる よう です が 、 その 頃 に は レンタル で て いる ので 、 今回 は わざわざ お金 だして 見る ほど で も ない です 。 xxbos 直感 で くだらな さ そう な 作品 だ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_83e7pMewJ_l"
   },
   "source": [
    "We will initialize our model with weights from a language model trained on Japanese Wikipedia. You can download them from [here](https://drive.google.com/open?id=1KRUEV_3R-JVhcftvWJ66rwU7e_EZWtxE) and you need to place the files in the `data/models/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axYJrS2jwJ_l"
   },
   "outputs": [],
   "source": [
    "#Google colab notes: upload model files\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#!cp /content/drive/My\\ Drive/ja-wiki-awdlstm-model/ja-wiki-itos.pkl \n",
    "#!cp /content/drive/My\\ Drive/ja-wiki-awdlstm-model/ja-wiki.pth ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKzpoyBl8aZi"
   },
   "outputs": [],
   "source": [
    "#!mv ja-wiki-itos.pkl data/models\n",
    "#!mv ja-wiki.pth data/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDROqQ_xwJ_n"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM,pretrained_fnames=['ja-wiki','ja-wiki-itos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8t935fGjwJ_p",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 04:03 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.375677</td>\n",
       "      <td>4.124529</td>\n",
       "      <td>0.292882</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1,1e-2,moms=(0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85HGj4ATwJ_w"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 37:12 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.069916</td>\n",
       "      <td>3.950010</td>\n",
       "      <td>0.312577</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.984557</td>\n",
       "      <td>3.836064</td>\n",
       "      <td>0.324826</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.870781</td>\n",
       "      <td>3.762562</td>\n",
       "      <td>0.333079</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.768262</td>\n",
       "      <td>3.712374</td>\n",
       "      <td>0.338276</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.714551</td>\n",
       "      <td>3.674273</td>\n",
       "      <td>0.342792</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.596981</td>\n",
       "      <td>3.647418</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.517474</td>\n",
       "      <td>3.636053</td>\n",
       "      <td>0.347601</td>\n",
       "      <td>04:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.468837</td>\n",
       "      <td>3.635810</td>\n",
       "      <td>0.347985</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8,3e-3,moms=(0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2kbsexYwJ_3"
   },
   "outputs": [],
   "source": [
    "learn.save('ymr')\n",
    "learn.save_encoder('ymr_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FarNDkEYwJ_7"
   },
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_df(mov_df_polar,path,cols=['text'],vocab=data_lm.vocab,processor=processor)\n",
    "                   .split_by_rand_pct(0.1)\n",
    "                   .label_from_df(cols=['rating'])\n",
    "                   .databunch(bs=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0F0IN2LwJ_9",
    "outputId": "20f1aebe-decc-4669-c64e-7dbed3d83285"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos ・ 過去 の 行い と 罪 の 意識 から 強 さ の 裏 で 自分 の 命 を 軽 ん じ て い た 剣心 が 、 師匠 と 剣 を 交える うち に 「 死に たく ない ！ 」 「 死ね ない ！ 」 と 強く 思い 、 「 生きよう と する 意思 」 に 目覚め た こと で 本当 の 強 さ を 手 に 入れ 、</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos 今年 は ジブリ が 輝い て い まし た 。 二 大 巨匠 の 宮崎駿 監督 と 高畑勲 監督 が 、 そろっ て 傑作 を 世に 出し た から でしょ う 。 さて 、 本作 、 物語 の 祖 と も 言わ れる 『 竹取物語 』 を モチーフ と し 、 展開 も ほぼ 原点 に 沿っ た もの と なっ て い ます 。 しかし 、 描か れ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos ターゲット の 夢 から 潜在意識 に 侵入 し 、 カタチ に なる 前 の アイデア を 盗み出す こと を 専門 と する 企業 スパイ 、 コブ （ ｌ ･ ディカプリオ ） は 、 業界 で も 屈指 の 凄腕 で ある 。 ある 事件 が 原因 で 最愛 の 妻 を 失い 、 その 殺人容疑 を かけ られ た 彼 は 、 祖国 に 二人 の 子供 を 残し</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos 私 が 思う ” 理想 的 な 悪人 俳優 ” は ショーン・ビーン と xxunk 、 そして xxunk です 。 という こと で 「 悪人 」 です 。 今作 は 「 パレード 」 で 知ら れる 吉田修一 さん の 原作 を 「 フラガール 」 や 「 69 」 を 手掛け た 李 監督 × 主演 に 妻夫木聡 さん × 音楽 を 久石譲 さん と 豪華 スタッフ ・ キャスト で 映像</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos 予告 トレーラー を 見 て ワクワク し つつ 、 反面 少し 不安 を 抱え ながら も 鑑賞 。 （ xxup 2d 字幕 ） しかし その 心配 は 杞憂 に 終わっ た ！ 本当に 素晴らしい 映画 だ ！ 新 世代 の 怪獣 xxup sf 映画 と 言える 快 作 （ 怪 作 ） だ 。 巨大 ロボ 「 イェーガー 」 は 無骨 で 重量感 たっぷり 、 まるで 巨大 な 重機</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Inr7kPswKAA"
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder('ymr_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7TS0BSNwKAF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 02:00 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.314499</td>\n",
       "      <td>0.251305</td>\n",
       "      <td>0.897328</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJAXCZlSwKAH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 02:14 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.271490</td>\n",
       "      <td>0.199659</td>\n",
       "      <td>0.923258</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PTdV7KbwKAI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 03:40 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.219154</td>\n",
       "      <td>0.183919</td>\n",
       "      <td>0.928497</td>\n",
       "      <td>03:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQrawItHwKAK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 09:43 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.193065</td>\n",
       "      <td>0.181743</td>\n",
       "      <td>0.931116</td>\n",
       "      <td>04:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180688</td>\n",
       "      <td>0.182315</td>\n",
       "      <td>0.931378</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('ymr_clas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfO83x3ywKAN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category 1, tensor(1), tensor([0.0094, 0.9906]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"映画すごかったよ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJDz8-28wKAP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category 0, tensor(0), tensor([0.9264, 0.0736]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"演技が悪い\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkXYHQhcwKAU"
   },
   "source": [
    "## Seq2Seq\n",
    "\n",
    "A sequence-to-sequence model is a model that takes a sequence of items and outputs another sequence of items using two networks that are trained end-to-end. This is perfect for machine translation since input sequences are directly related to output sequences. We will looking at preparing a dataset for Machine Translation task and implementing a seq2seq model. We will be using the parallel corpus available from [here](ftp://ftp.monash.edu/pub/nihongo/examples.utf.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5KcBSgDwKAU"
   },
   "outputs": [],
   "source": [
    "def make_corpus(corpus_path):\n",
    "    corpus = corpus_path.open().readlines()\n",
    "    data_pair = []\n",
    "    pat = r'#ID.+\\n'\n",
    "    for c in corpus:\n",
    "        if 'A: ' in c:\n",
    "            clean_c = c.replace('A: ','')\n",
    "            res = re.search(pat,clean_c)\n",
    "            clean_c = clean_c.replace(res.group(0),'').split('\\t')\n",
    "            data_pair.append((clean_c[0],clean_c[1]))\n",
    "    return pd.DataFrame(data_pair,columns=['ja','en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBvnts3UwKAY",
    "outputId": "82c45ceb-1eae-44e3-e742-0d0ca8e2fb65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ja</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ムーリエルは２０歳になりました。</td>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>すぐに戻ります。</td>\n",
       "      <td>I will be back soon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>すぐに諦めて昼寝をするかも知れない。</td>\n",
       "      <td>I may give up soon and just nap instead.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>愛してる。</td>\n",
       "      <td>I love you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ログアウトするんじゃなかったよ。</td>\n",
       "      <td>I shouldn't have logged off.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ja                                        en\n",
       "0    ムーリエルは２０歳になりました。                        Muiriel is 20 now.\n",
       "1            すぐに戻ります。                      I will be back soon.\n",
       "2  すぐに諦めて昼寝をするかも知れない。  I may give up soon and just nap instead.\n",
       "3               愛してる。                               I love you.\n",
       "4    ログアウトするんじゃなかったよ。              I shouldn't have logged off."
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = make_corpus(path/'examples.utf')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Kvee5X9wKAa",
    "outputId": "3890af75-7119-4f9d-bccf-0d1c27a442e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149784"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9PxAEtXwKAd",
    "outputId": "e75f4df0-9e53-4bbd-fc86-daf64ff6a1c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149784"
      ]
     },
     "execution_count": 286,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_idxs = np.random.choice(np.arange(len(df)), int(0.1*len(df)),replace=False)\n",
    "train_idxs = [i for i in np.arange(len(df)) if i not in valid_idxs]\n",
    "len(train_idxs) + len(valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1UYkzBUwKAg"
   },
   "outputs": [],
   "source": [
    "#collation function and databunch code borrowed from here: \n",
    "#https://github.com/ohmeow/seq2seq-pytorch-fastai/blob/master/seq2seq-rnn-attn.ipynb\n",
    "\n",
    "def seq2seq_pad_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=False, \n",
    "                        include_targets=True, include_lengths=False, include_masks=False,\n",
    "                        backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    \n",
    "    samples = to_data(samples)\n",
    "    samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    x_lens = [len(s[0]) for s in samples]\n",
    "    x_max_len = max(x_lens)\n",
    "    x_res = torch.zeros(len(samples), x_max_len).long() + pad_idx\n",
    "    \n",
    "    y_lens = [len(s[1]) for s in samples]\n",
    "    y_max_len = max(y_lens)\n",
    "    y_res = torch.zeros(len(samples), y_max_len).long() + pad_idx\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "        \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            x_res[i,-len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,-len(s[1]):] = LongTensor(s[1])\n",
    "        else:         \n",
    "            x_res[i,:len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,:len(s[1]):] = LongTensor(s[1])\n",
    "            \n",
    "    if backwards: res = res.flip(1)\n",
    "        \n",
    "    x = [x_res]\n",
    "    if (include_targets): x += [y_res.clone()]\n",
    "    if (include_lengths): x += [torch.tensor(x_lens), torch.tensor(y_lens)]\n",
    "    if (include_masks): x += [x_res != pad_idx, y_res != pad_idx]\n",
    "    \n",
    "    return x, y_res\n",
    "\n",
    "\n",
    "class Seq2SeqDataBunch(DataBunch):\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, \n",
    "               path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1, pad_first=False, \n",
    "               device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:\n",
    "        \n",
    "        \"\"\"Function that transform the `datasets` in a `DataBunch` for seq2seq task. \n",
    "        Passes `**dl_kwargs` on to `DataLoader()`\"\"\"\n",
    "        \n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        \n",
    "        collate_fn = partial(seq2seq_pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        \n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        \n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9_Q7LXJwKAi"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpOxgbMTwKAk"
   },
   "outputs": [],
   "source": [
    "en_tok = Tokenizer(lang='en')\n",
    "en_procs = [TokenizeProcessor(tokenizer=en_tok, include_bos=True, include_eos=True), \n",
    "            NumericalizeProcessor(min_freq=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jo-p4T_pwKAm"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(MeCabTokenizer, 'ja')\n",
    "ja_procs = [TokenizeProcessor(tokenizer=tokenizer,include_bos=True, include_eos=True), NumericalizeProcessor(max_vocab=30000,min_freq=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3WuE-cawKAn"
   },
   "outputs": [],
   "source": [
    "en_il = Seq2SeqTextList.from_df(df, path, cols=['en'], processor=en_procs).process().split_by_idxs(train_idxs,valid_idxs)\n",
    "ja_il = Seq2SeqTextList.from_df(df, path, cols=['ja'], processor=ja_procs).process().split_by_idxs(train_idxs,valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lp77piH5wKAp"
   },
   "outputs": [],
   "source": [
    "tr_ll = LabelList(en_il.train,ja_il.train)\n",
    "val_ll = LabelList(en_il.valid,ja_il.valid)\n",
    "lls = LabelLists(path,train=tr_ll,valid=val_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HU6eew3wKAr"
   },
   "outputs": [],
   "source": [
    "seq2seq_data = lls.databunch(bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgDqNCjywKAs",
    "outputId": "38610cfa-b5cc-4ad0-9d41-dd8896cba213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj muiriel is 20 now . xxeos,\n",
       " Text xxbos ムーリエル は ２ ０ 歳 に なり まし た 。 xxeos)"
      ]
     },
     "execution_count": 291,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_data.train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XejAD33ZwKAu"
   },
   "outputs": [],
   "source": [
    "en_vecs,_,dim_en_vec = load_vectors('data/wiki-news-300d-1M.vec')\n",
    "j_vecs,_,dim_j_vec = load_vectors('data/cc.ja.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pydn2l1bwKAw"
   },
   "outputs": [],
   "source": [
    "def create_emb(vecs, itos, em_sz):\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    if vecs is None: return emb\n",
    "    wgts = emb.weight.data\n",
    "    miss = []\n",
    "    for i,w in enumerate(itos):\n",
    "        try: wgts[i] = torch.from_numpy(vecs[w])\n",
    "        except: miss.append(w)\n",
    "    print('Number of unknowns in data: {}'.format(len(miss)))\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1roMu9AwKAx"
   },
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCIfiPkVwKA2"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self,int2en,int2j,em_sz,j_vecs=None,en_vecs=None,nh=128,out_sl=25,dropf=1,nl=2):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        self.nl,self.nh,self.em_sz,self.out_sl = nl,nh,em_sz,out_sl\n",
    "        self.emb_enc = create_emb(en_vecs,int2en,em_sz)\n",
    "        self.emb_drop = nn.Dropout(0.15*dropf)\n",
    "        self.encoder = nn.GRU(em_sz,nh,num_layers=nl,dropout=0.25*dropf, bidirectional=True,batch_first=True)\n",
    "        #decoder\n",
    "        self.emb_dec = create_emb(j_vecs,int2j,em_sz)\n",
    "        self.decoder = nn.GRU(em_sz,nh*2,num_layers=nl,dropout=0.25*dropf,batch_first=True)\n",
    "        self.out_drop = nn.Dropout(0.35*dropf)\n",
    "        self.out = nn.Linear(nh*2,len(int2j))\n",
    "        #attention layer\n",
    "        self.W1 = rand_p(nh*2, nh*2) #parameter\n",
    "        self.l2 = nn.Linear(nh*2, nh*2)\n",
    "        self.l3 = nn.Linear(em_sz+nh*2, em_sz)\n",
    "        self.V = rand_p(nh*2) #parameter\n",
    "        self.targets = None\n",
    "    \n",
    "    def forward(self,inp,y=None):\n",
    "        self.targets = y\n",
    "        bs,sl = inp.size()\n",
    "        emb_in = self.emb_drop(self.emb_enc(inp))\n",
    "        h_n = self.initHidden(bs)\n",
    "        enc_out, h_n = self.encoder(emb_in,h_n)\n",
    "        h_n = h_n.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(self.nl,bs,-1)\n",
    "        \n",
    "        dec_inp = torch.zeros(bs).long().cuda()\n",
    "        res,attns = [], []\n",
    "        #multiply by parameter\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            #linear layer \n",
    "            w2h = self.l2(h_n[-1])\n",
    "            #non-linear activation to calculate score\n",
    "            u = torch.tanh(w1e + w2h.unsqueeze(1))\n",
    "            #softmax to make them into probs\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a)\n",
    "            #multiply each vector by scores and then add them up\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(1)\n",
    "            dec_emb = self.emb_dec(dec_inp)\n",
    "            #linear layer to reduce dimensions\n",
    "            wgt_enc = self.l3(torch.cat([dec_emb, Xa], 1))\n",
    "            outp,h_n = self.decoder(wgt_enc.unsqueeze(1),h_n)\n",
    "            outp = self.out(self.out_drop(outp[:,0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = outp.data.max(1)[1]\n",
    "            if i >=self.targets.size(1):break\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (random.random() > 0.5) and self.targets is not None: dec_inp=y[:,i] \n",
    "        return torch.stack(res).transpose(1,0)\n",
    "        \n",
    "    def initHidden(self,bs):\n",
    "        return torch.zeros([self.nl*2,bs,self.nh]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgE3MgKewKA4"
   },
   "outputs": [],
   "source": [
    "def seq2seq_loss(input, target):\n",
    "    bs,sl = target.size()\n",
    "    bs_in,sl_in,nc = input.size()\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,sl-sl_in,0,0))\n",
    "    input = input[:,:sl,:]\n",
    "    return F.cross_entropy(input.contiguous().view(-1,nc), target.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFmsFBajwKA6"
   },
   "outputs": [],
   "source": [
    "class TeacherForcingCallback(LearnerCallback):\n",
    "    learn:Learner\n",
    "        \n",
    "    def on_batch_begin(self, train, **kwargs):\n",
    "        learn.model.targets = kwargs['last_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkG597I4wKA9",
    "outputId": "3ef8c0a6-3dc7-4e61-eb65-6cdf9bb13279"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqAttention(\n",
       "  (emb_enc): Embedding(21395, 300, padding_idx=1)\n",
       "  (emb_drop): Dropout(p=0.15)\n",
       "  (encoder): GRU(300, 128, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  (emb_dec): Embedding(30004, 300, padding_idx=1)\n",
       "  (decoder): GRU(300, 256, num_layers=2, batch_first=True, dropout=0.25)\n",
       "  (out_drop): Dropout(p=0.35)\n",
       "  (out): Linear(in_features=256, out_features=30004, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=556, out_features=300, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 295,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq = Seq2SeqAttention(seq2seq_data.train_ds.x.vocab.itos,seq2seq_data.train_ds.y.vocab.itos,300,en_vecs=None,j_vecs=None)\n",
    "seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tco_NyzwKBA"
   },
   "outputs": [],
   "source": [
    "learn = Learner(seq2seq_data,seq2seq)\n",
    "learn.loss_func = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Pt8Oc0awKBD"
   },
   "outputs": [],
   "source": [
    "learn.callbacks.append(TeacherForcingCallback(learn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwqdx2n7wKBH",
    "outputId": "8e357689-3322-40af-c1a9-b3914fbab3f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCzEk-vPwKBK",
    "outputId": "03288ac5-1813-4a03-e6f7-4610ccfb2bf4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyZC6HMdwKBP",
    "outputId": "e04024bf-f3f0-48e4-aef6-0bd10d6f9bd4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 3e-3, moms=(0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LkXYHQhcwKAU",
    "b0_QNFnPwKBT"
   ],
   "include_colab_link": true,
   "name": "Copy of fastai-text-data-examples.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
